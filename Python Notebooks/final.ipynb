{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final.ipynb","provenance":[],"authorship_tag":"ABX9TyNli4393gEJcYx681ISjebM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"I_6pfFxxrf7k","executionInfo":{"status":"ok","timestamp":1619439548759,"user_tz":-330,"elapsed":5236,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["#Importing Liraries\n","\n","# Importing Libraries\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from tqdm import tqdm\n","import shutil\n","import os\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","import matplotlib\n","matplotlib.use(u'nbAgg')\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","import pickle\n","import random\n","import joblib\n","from scipy.stats import randint as sp_randint\n","from scipy.stats import uniform\n","from scipy.sparse import hstack\n","from wordcloud import WordCloud\n","\n","\n","# Utilities\n","#from viz_utils import *\n","#from custom_transformers import *\n","#from ml_utils import *\n","\n","# DataPrep\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import RSLPStemmer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import Normalizer\n","\n","# Modeling\n","\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.linear_model import SGDClassifier,LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier, AdaBoostClassifier\n","#Metrics\n","from sklearn.metrics import log_loss,accuracy_score, confusion_matrix, f1_score\n","\n","\n","\n","\n","\n","#Importing the Libraries\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.models import load_model\n","\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yw51y0cCrf-e","executionInfo":{"status":"ok","timestamp":1619439551403,"user_tz":-330,"elapsed":5093,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"4cc6e879-3e3f-407f-df7c-aa401f0c1264"},"source":["import time\n","start = time.time()\n","data = pd.read_csv(\"olist_customers_dataset.csv\")\n","geo_data = pd.read_csv(\"olist_geolocation_dataset.csv\")\n","order_itemdata = pd.read_csv(\"olist_order_items_dataset.csv\")\n","pay_data = pd.read_csv(\"olist_order_payments_dataset.csv\")\n","rev_data = pd.read_csv(\"olist_order_reviews_dataset.csv\")\n","orders = pd.read_csv(\"olist_orders_dataset.csv\")\n","order_prddata = pd.read_csv(\"olist_products_dataset.csv\")\n","order_selldata = pd.read_csv(\"olist_sellers_dataset.csv\")\n","order_prd_catdata = pd.read_csv(\"product_category_name_translation.csv\")\n","end = time.time()\n","print(\"reading time: \",(end-start),\"sec\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["reading time:  2.460000514984131 sec\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wkP7PRi8rgB6","executionInfo":{"status":"ok","timestamp":1619439552041,"user_tz":-330,"elapsed":3927,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["#merging data\n","rev_new = rev_data.drop(['review_comment_title','review_creation_date','review_id','review_answer_timestamp'],axis=1)\n","df = pd.merge(orders,pay_data, on=\"order_id\")\n","df = df.merge(data, on=\"customer_id\")\n","df = df.merge(order_itemdata, on=\"order_id\")\n","df = df.merge(order_prddata, on=\"product_id\")\n","df = df.merge(order_prd_catdata, on=\"product_category_name\")\n","df = df.merge(rev_new, on=\"order_id\")\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"pZXfdOm4vRmW","executionInfo":{"status":"ok","timestamp":1619439552435,"user_tz":-330,"elapsed":1664,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"5037d603-afdd-4285-aa30-519b3a95dcad"},"source":["#sellers count for each product\n","fea_1= df.groupby('product_id').count()['seller_id']\n","fea_1_df = pd.DataFrame()\n","fea_1_df['product_id']= fea_1.index\n","fea_1_df['sellers_count']= fea_1.values\n","fea_1_df.head()    "],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_id</th>\n","      <th>sellers_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00066f42aeeb9f3007548bb9d3f33c38</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00088930e925c41fd95ebfe695fd2655</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0009406fd7479715e4bef61dd91f2462</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>000b8f95fcb9e0096488278317764d19</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>000d9be29b5207b54e86aa1b1ac54872</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         product_id  sellers_count\n","0  00066f42aeeb9f3007548bb9d3f33c38              1\n","1  00088930e925c41fd95ebfe695fd2655              1\n","2  0009406fd7479715e4bef61dd91f2462              1\n","3  000b8f95fcb9e0096488278317764d19              2\n","4  000d9be29b5207b54e86aa1b1ac54872              1"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"3S2cmykhvbvh","executionInfo":{"status":"ok","timestamp":1619439554305,"user_tz":-330,"elapsed":1896,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"d6ed5e3d-5418-4cee-a795-67bc90850160"},"source":["#sellers count for each product\n","fea_2 = df.groupby('order_id').count()['product_id']\n","fea_2_df = pd.DataFrame()\n","fea_2_df['order_id']= fea_2.index\n","fea_2_df['products_count']= fea_2.values\n","fea_2_df.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>order_id</th>\n","      <th>products_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00018f77f2f0320c557190d7a144bdd3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000229ec398224ef6ca0657da4fc703e</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00024acbcdf0a6daa1e931b038114c75</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           order_id  products_count\n","0  00010242fe8c5a6d1ba2dd792cb16214               1\n","1  00018f77f2f0320c557190d7a144bdd3               1\n","2  000229ec398224ef6ca0657da4fc703e               1\n","3  00024acbcdf0a6daa1e931b038114c75               1\n","4  00042b26cf59d7ce69dfabb4e55b4fd9               1"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"hqYzQ_sSvt3P","executionInfo":{"status":"ok","timestamp":1619439556131,"user_tz":-330,"elapsed":1911,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["# Adding the seller count and products count feature to the final data set\n","df = pd.merge(df,fea_1_df,on='product_id')\n","df = pd.merge(df,fea_2_df,on='order_id')\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oYbp-mHns2y","executionInfo":{"status":"ok","timestamp":1619439562468,"user_tz":-330,"elapsed":844,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"5a04c4f3-58a1-417d-a90d-2d7299fbec7c"},"source":["# separating the target variable\n","y = df['review_score']\n","X = df.drop(labels='review_score',axis=1)\n","\n","# train test 80:20 split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=25)\n","print(\"Train data: \",X_train.shape,y_train.shape)\n","print(\"Train data: \",X_test.shape,y_test.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Train data:  (93264, 34) (93264,)\n","Train data:  (23317, 34) (23317,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZdNbQaTa0QIy","executionInfo":{"status":"ok","timestamp":1619439564086,"user_tz":-330,"elapsed":820,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"85c88ffb-2b97-4cf6-9020-4be77048e1b1"},"source":["#text data preprocessing\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('rslp')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Package rslp is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5vFl8RfCdJw","executionInfo":{"status":"ok","timestamp":1619439753971,"user_tz":-330,"elapsed":187542,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"b9e3fbda-5da1-411f-c224-ab0d461efba8"},"source":["from gensim.models import FastText\n","ft_model = FastText.load_fasttext_format('/content/drive/MyDrive/case study/cc.pt.300.bin')\n","print(ft_model.wv['melhor'].shape)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["(300,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1wOmXuAJowlf","executionInfo":{"status":"ok","timestamp":1619439887561,"user_tz":-330,"elapsed":875,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["def tfidfWord2Vector(text,ft_words,tfidf_words,tf_values):\n","    # average Word2Vec\n","    # compute average word2vec for each review.\n","    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n","    for sentence in tqdm(text): # for each review/sentence\n","        vector = np.zeros(300) # as word vectors are of zero length\n","        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","        for word in sentence.split(): # for each word in a review/sentence\n","            if (word in ft_words) and (word in tfidf_words):\n","                vec = ft_model.wv[word] # embeddings[word] \n","                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","                tf_idf = tf_values[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n","                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","                tf_idf_weight += tf_idf\n","        if tf_idf_weight != 0:\n","            vector /= tf_idf_weight\n","        tfidf_w2v_vectors.append(vector)\n","    tfidf_w2v_vectors = np.asarray(tfidf_w2v_vectors)\n","    \n","    return tfidf_w2v_vectors\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAdsVvNbaCfc","executionInfo":{"status":"ok","timestamp":1619439889703,"user_tz":-330,"elapsed":1215,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"50283af1-6db4-4547-b16f-688908b9d3e5"},"source":["#importing func.py \n","from func import *"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Package rslp is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5a0-dO3Xxoky","executionInfo":{"status":"ok","timestamp":1619439894147,"user_tz":-330,"elapsed":1374,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["def preprocessing(X):\n","  #replacing null values\n","  X = replace_nan(X)\n","\n","  #dedublication\n","  X,index = dedublicate(X)\n","  \n","  timestamp_col = ['order_purchase_timestamp','order_approved_at','order_delivered_customer_date',\n","                     'order_estimated_delivery_date']\n","  X[timestamp_col]= X[timestamp_col].apply(pd.to_datetime)\n","  #adding new features \n","  X = feat_engg(X)\n","  #rfm_lvl = rfm_level()\n","  rfm = rfm_feat(X)\n","  X = X.merge(rfm ,on ='customer_unique_id',how='left')    \n","\n","  #dropping columns     \n","  col= ['order_id','customer_id','order_purchase_timestamp','order_approved_at','order_delivered_customer_date',\n","  'order_estimated_delivery_date','customer_unique_id','order_item_id','product_id','seller_id','shipping_limit_date','f_quartile','r_quartile',\n","  'm_quartile','RFM_Score','RFM_Score_s','product_category_name']\n","  X.drop(columns=col,axis=1,inplace=True)\n","\n","  #text preprocessing\n","  process_txt  = preprocess_text(X['review_comment_message'])\n","  X['review_comment_message'] = process_txt\n","  \n","  #TEXT featurization\n","\n","\n","  # encoding review comment message using Tfidf weighted W2V\n","  tfidf = TfidfVectorizer()\n","  tfidf.fit(X['review_comment_message'])\n","  \n","  # we are converting a dictionary with word as a key, and the idf as a value\n","  tf_values = dict(zip(tfidf.get_feature_names(), list(tfidf.idf_)))\n","  tfidf_words = set(tfidf.get_feature_names())\n","  ft_words = list(ft_model.wv.vocab.keys()) # list(embeddings.keys())\n","  tfidf_w2v_vectors_X = tfidfWord2Vector(X['review_comment_message'].values,ft_words,tfidf_words,tf_values)\n","\n","  cat_col = ['order_status','payment_type','customer_state','product_category_name_english','RFM_Level']\n","  X_train = pickle.load(open('X_train.pkl','rb')) \n","  cat_feat = cat_feats(cat_col,X_train,X)\n","\n","  # numerical features\n","  num=['payment_sequential','payment_installments','payment_value','customer_zip_code_prefix','price',\n","  'freight_value','product_name_lenght','product_description_lenght','product_photos_qty',\n","  'product_weight_g','product_length_cm','product_height_cm','product_width_cm',\n","  'recency','frequency','monetary','sellers_count','products_count','est_delivery_t',\n","  'act_delivery_t','diff_in_delivery_t','on_time_delivery','avg_prdt_value','total_order_cost',\n","  'order_freight_ratio','purchase_dayofweek','is_reviewed','words_per_review','day_to_delivery']\n","   \n","  X_num = num_feats(X,num)\n"," \n","  #tokenization and pad_sequencing \n","  textX = create_tokenizer(X_train['review_comment_message'],X['review_comment_message'])\n","  '''input = []\n","  for cat in cat_col:\n","    pad = create_tokenizer(X_train[cat],X[cat])\n","    input.append(pad)\n","  input = list(textX)+input'''\n","  trainX_os = create_tokenizer(X_train['order_status'],X['order_status'])\n","  trainX_pt = create_tokenizer(X_train['payment_type'],X['payment_type'])\n","  trainX_st = create_tokenizer(X_train['customer_state'],X['customer_state'])\n","  trainX_pc = create_tokenizer(X_train['product_category_name_english'],X['product_category_name_english'])\n","  trainX_rfm = create_tokenizer(X_train['RFM_Level'],X['RFM_Level'])\n","  x_train=[textX, trainX_os, trainX_pt, trainX_st, trainX_pc, trainX_rfm, X_num]\n","  return tfidf_w2v_vectors_X,cat_feat,X_num,textX,x_train,index"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBUJyz8JrpzN","executionInfo":{"status":"ok","timestamp":1619444076245,"user_tz":-330,"elapsed":1607,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["from scipy.sparse import hstack\n","def function_1(X):\n","    tfidf_w2v_vectors_X,cat_feat,X_num,textX,x_train,index = preprocessing(X)\n","    X_tr = hstack((tfidf_w2v_vectors_X,list(cat_feat.values())[0],list(cat_feat.values())[1],list(cat_feat.values())[2],\n","                    list(cat_feat.values())[3],list(cat_feat.values())[4],X_num)).tocsr()\n","    # load the model from file\n","    encoder = load_model('encoder.h5')\n","\n","    # encode the train data\n","    X_encode = encoder.predict(X_tr)\n","    X_encode_1 = X_encode.reshape(X_encode.shape[0],X_encode.shape[1],1)\n","\n","\n","    # merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n","\n","    X_tr_num = hstack((list(cat_feat.values())[0],list(cat_feat.values())[1],list(cat_feat.values())[2],\n","                    list(cat_feat.values())[3],list(cat_feat.values())[4],X_num)).tocsr()\n","\n","    x_tr_num = np.array(X_tr_num.todense()).reshape(X_tr_num.shape[0],X_tr_num.shape[1],1)\n","        \n","    #loading models\n","    model_1 = load_model('models/model_1.h5')\n","    model_2 = load_model('models/model_2.h5')\n","    model_3 = load_model('models/model_3.h5')\n","    model_4 = load_model('models/model_4.h5')\n","    model_5 = load_model('models/model_5.h5')\n","\n","    #saving the model\n","    filename = '/content/drive/MyDrive/case study/models/stacknn2.sav'\n","    stacknn2 = joblib.load(filename)\n","\n","    \n","    #prediction of train data\n","    y_pred1 = model_1.predict(X_encode)\n","    y_pred2 = model_2.predict(X_encode_1)\n","    y_pred3 = model_3.predict(x_train)\n","    y_pred4 = model_4.predict([textX,x_tr_num])\n","    y_pred5 = model_5.predict([textX,X_tr_num])\n","    \n","\n","    y_pred = stacknn2.predict(np.stack((np.greater(y_pred1,0.5).astype(int)[:,0],\n","                                                np.greater(y_pred2,0.5).astype(int)[:,0],\n","                                                np.greater(y_pred3,0.5).astype(int)[:,0],\n","                                                np.greater(y_pred4,0.5).astype(int)[:,0],\n","                                                np.greater(y_pred5,0.5).astype(int)[:,0]),axis=-1))\n","    \n","    return y_pred,index"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sz5x4Z4F50nj","executionInfo":{"status":"ok","timestamp":1619444080935,"user_tz":-330,"elapsed":957,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["def function_2(X,y):\n","  y_pred,index = func_1(X)\n","  y = y.apply(lambda x:1 if x>3 else 0)\n","  y = y.drop(index=index,axis=0)\n","  return f1_score(y,y_pred,average='macro')"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"NqilxSOcUoeI","executionInfo":{"status":"ok","timestamp":1619442967795,"user_tz":-330,"elapsed":908,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}}},"source":["import logging\n","logging.getLogger('tensorflow').disabled = True"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdAGKsW56wn5","executionInfo":{"status":"ok","timestamp":1619446013979,"user_tz":-330,"elapsed":1870450,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"f42d6cc9-41b0-45d7-aba4-96928417a8cb"},"source":["%%time\n","function_2(X_test,y_test)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["100%|██████████| 22161/22161 [00:05<00:00, 3955.68it/s]\n","100%|██████████| 22161/22161 [29:47<00:00, 12.40it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 31min 29s, sys: 16.3 s, total: 31min 45s\n","Wall time: 31min 9s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.7766569587113161"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j2OMLwNg9fmH","executionInfo":{"status":"ok","timestamp":1619446094199,"user_tz":-330,"elapsed":62821,"user":{"displayName":"Paritosh Mahto","photoUrl":"","userId":"05081380420602849805"}},"outputId":"e2705cd9-9496-436e-d9f0-6ff124c5e7bb"},"source":["%%time\n","function_2(X.iloc[300:1000],y.iloc[300:1000])"],"execution_count":51,"outputs":[{"output_type":"stream","text":["100%|██████████| 605/605 [00:00<00:00, 4046.39it/s]\n","100%|██████████| 605/605 [00:47<00:00, 12.64it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 1min 1s, sys: 636 ms, total: 1min 2s\n","Wall time: 1min 1s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.7484158415841584"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"8lYKwWmBgWNy"},"source":[""],"execution_count":null,"outputs":[]}]}